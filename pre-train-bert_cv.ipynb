{"cells":[{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"editable":false,"id":"e6qg-tZRAs5o","outputId":"ed86966e-300a-4c0b-f87c-cff04957f545"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!"]},{"data":{"text/plain":["True"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import KFold\n","from transformers import (\n","    BertTokenizer,\n","    DataCollatorForLanguageModeling,\n","    BertForPreTraining,\n","    Trainer,\n","    TrainingArguments,\n","    EarlyStoppingCallback,\n",")\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from datasets import Dataset\n","import random\n","import evaluate\n","import logging\n","import torch\n","import nltk\n","from datasets import Dataset, DatasetDict\n","import matplotlib.pyplot as plt\n","import os\n","import shutil\n","nltk.download('punkt_tab')"]},{"cell_type":"code","execution_count":39,"metadata":{"editable":false,"id":"DyfV8PPXYZv2"},"outputs":[],"source":["RANDOM_SEED = 42\n","\n","BLOCK_SIZE = 256  # Maximum number of tokens in an input sample\n","NSP_PROB = 0.50  # Probability that the next sentence is the actual next sentence in NSP ##=> Segun BERT\n","MAX_LENGTH = 512  # Maximum number of tokens in an input sample after padding\n","SHORT_SEQ_PROB = 0.1  # Probability of generating shorter sequences to minimize the mismatch between pretraining and fine-tuning.\n","MLM_PROB = 0.15  # Probability with which tokens are masked in MLM ##=> BERTimbau 0.15\n","\n","# Entrenaniento\n","TRAIN_BATCH_SIZE = 128  # Batch-size for pretraining the model on ##=> BERTimbau base 128\n","EVAL_BATCH_SIZE = 128\n","MAX_EPOCHS = 40  # Maximum number of epochs to train the model for\n","WEIGHT_DECAY = 0.01\n","\n","# Early stopping\n","ES_PATIENCE = 3\n","ES_THRESHOLD = 0.001\n","\n","PATH_DATASET = \"unsupervised.csv\"\n","PATH_RESULT_MODEL = \"/vol/model\"\n","\n","# Modelo\n","# Name of pretrained model from ðŸ¤— Model Hub\n","MODEL_CHECKPOINT = \"neuralmind/bert-base-portuguese-cased\"\n","\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","# Set up logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","metric = evaluate.load(\"accuracy\")"]},{"cell_type":"code","execution_count":7,"metadata":{"editable":false,"id":"NI-mBfl0Away"},"outputs":[],"source":["def load_and_prepare_model(model_name=\"neuralmind/bert-base-portuguese-cased\"):\n","    # Load pre-trained model and tokenizer\n","    model = BertForPreTraining.from_pretrained(model_name)\n","    tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","    return model, tokenizer"]},{"cell_type":"code","execution_count":8,"metadata":{"editable":false,"id":"WdfJeWkVA0Xy"},"outputs":[],"source":["\n","def prepare_train_features(examples, tokenizer):\n","\n","    #######\n","    # NSP\n","    # We define the maximum number of tokens after tokenization that each training sample\n","    # will have\n","    max_num_tokens = BLOCK_SIZE - \\\n","        tokenizer.num_special_tokens_to_add(pair=True)\n","\n","    \"\"\"Function to prepare features for NSP task\n","\n","    Arguments:\n","      examples: A dictionary with 1 key (\"text\")\n","        text: List of raw documents (str)\n","    Returns:\n","      examples:  A dictionary with 4 keys\n","        input_ids: List of tokenized, concatnated, and batched\n","          sentences from the individual raw documents (int)\n","        token_type_ids: List of integers (0 or 1) corresponding\n","          to: 0 for senetence no. 1 and padding, 1 for sentence\n","          no. 2\n","        attention_mask: List of integers (0 or 1) corresponding\n","          to: 1 for non-padded tokens, 0 for padded\n","        next_sentence_label: List of integers (0 or 1) corresponding\n","          to: 1 if the second sentence actually follows the first,\n","          0 if the senetence is sampled from somewhere else in the corpus\n","    \"\"\"\n","\n","    # Remove un-wanted samples from the training set\n","    examples[\"document\"] = [\n","        d.strip() for d in examples[\"text\"] if len(d) > 0\n","    ]\n","    # Split the documents from the dataset into it's individual sentences\n","    examples[\"sentences\"] = [\n","        nltk.tokenize.sent_tokenize(document) for document in examples[\"document\"]\n","    ]\n","    # Convert the tokens into ids using the trained tokenizer\n","    examples[\"tokenized_sentences\"] = [\n","        [tokenizer.convert_tokens_to_ids(\n","            tokenizer.tokenize(sent)) for sent in doc]\n","        for doc in examples[\"sentences\"]\n","    ]\n","\n","    # Define the outputs\n","    examples[\"input_ids\"] = []\n","    examples[\"token_type_ids\"] = []\n","    examples[\"attention_mask\"] = []\n","    examples[\"next_sentence_label\"] = []\n","\n","    for doc_index, document in enumerate(examples[\"tokenized_sentences\"]):\n","\n","        current_chunk = []  # a buffer stored current working segments\n","        current_length = 0\n","        i = 0\n","\n","        # We *usually* want to fill up the entire sequence since we are padding\n","        # to `block_size` anyways, so short sequences are generally wasted\n","        # computation. However, we *sometimes*\n","        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n","        # sequences to minimize the mismatch between pretraining and fine-tuning.\n","        # The `target_seq_length` is just a rough target however, whereas\n","        # `block_size` is a hard limit.\n","        target_seq_length = max_num_tokens\n","\n","        # If your sentences are very long, you might want to adjust these:\n","        # Increase minimum length for \"short\" sequences\n","        target_seq_length = random.randint(2, max_num_tokens)\n","        \n","        while i < len(document):\n","            segment = document[i]\n","            current_chunk.append(segment)\n","            current_length += len(segment)\n","            if i == len(document) - 1 or current_length >= target_seq_length:\n","                if current_chunk:\n","                    # `a_end` is how many segments from `current_chunk` go into the `A`\n","                    # (first) sentence.\n","                    a_end = 1\n","                    if len(current_chunk) >= 2:\n","                        a_end = random.randint(1, len(current_chunk) - 1)\n","\n","                    tokens_a = []\n","                    for j in range(a_end):\n","                        tokens_a.extend(current_chunk[j])\n","\n","                    tokens_b = []\n","\n","                    if len(current_chunk) == 1 or random.random() < NSP_PROB:\n","                        is_random_next = True\n","                        target_b_length = target_seq_length - len(tokens_a)\n","\n","                        # This should rarely go for more than one iteration for large\n","                        # corpora. However, just to be careful, we try to make sure that\n","                        # the random document is not the same as the document\n","                        # we're processing.\n","                        for _ in range(10):\n","                            random_document_index = random.randint(\n","                                0, len(examples[\"tokenized_sentences\"]) - 1\n","                            )\n","                            if random_document_index != doc_index:\n","                                break\n","\n","                        random_document = examples[\"tokenized_sentences\"][\n","                            random_document_index\n","                        ]\n","                        random_start = random.randint(\n","                            0, len(random_document) - 1)\n","                        for j in range(random_start, len(random_document)):\n","                            tokens_b.extend(random_document[j])\n","                            if len(tokens_b) >= target_b_length:\n","                                break\n","                        # We didn't actually use these segments so we \"put them back\" so\n","                        # they don't go to waste.\n","                        num_unused_segments = len(current_chunk) - a_end\n","                        i -= num_unused_segments\n","                    else:\n","                        is_random_next = False\n","                        for j in range(a_end, len(current_chunk)):\n","                            tokens_b.extend(current_chunk[j])\n","\n","                    # Controlar que los tokens no exedan 512\n","                    if len(tokens_a) > max_num_tokens:\n","                        tokens_a = tokens_a[0:max_num_tokens]\n","                    if len(tokens_b) > max_num_tokens:\n","                        tokens_b = tokens_b[0:max_num_tokens]\n","\n","                    input_ids = tokenizer.build_inputs_with_special_tokens(\n","                        tokens_a, tokens_b\n","                    )\n","                    # add token type ids, 0 for sentence a, 1 for sentence b\n","                    token_type_ids = tokenizer.create_token_type_ids_from_sequences(\n","                        tokens_a, tokens_b\n","                    )\n","\n","                    padded = tokenizer.pad(\n","                        {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids},\n","                        padding=\"max_length\",\n","                        max_length=MAX_LENGTH,\n","                    )\n","\n","                    examples[\"input_ids\"].append(padded[\"input_ids\"])\n","                    examples[\"token_type_ids\"].append(padded[\"token_type_ids\"])\n","                    examples[\"attention_mask\"].append(padded[\"attention_mask\"])\n","                    examples[\"next_sentence_label\"].append(\n","                        1 if is_random_next else 0)\n","                    current_chunk = []\n","                    current_length = 0\n","            i += 1\n","\n","    # We delete all the un-necessary columns from our dataset\n","    del examples[\"document\"]\n","    del examples[\"sentences\"]\n","    del examples[\"text\"]\n","    del examples[\"tokenized_sentences\"]\n","\n","    return examples"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_cv_datasets(df, n_splits=5, random_state=RANDOM_SEED):\n","    \"\"\"Create cross-validation datasets\"\"\"\n","    \n","    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n","    texts = df['text'].values\n","    \n","    cv_datasets = []\n","    for fold, (train_idx, val_idx) in enumerate(kf.split(texts)):\n","        x_train = pd.Series(texts[train_idx])\n","        x_val = pd.Series(texts[val_idx])\n","        \n","        # Convert to datasets\n","        train_dataset = Dataset.from_pandas(pd.DataFrame(x_train, columns=['text']))\n","        val_dataset = Dataset.from_pandas(pd.DataFrame(x_val, columns=['text']))\n","        \n","        # Create DatasetDict\n","        dataset_dict = DatasetDict({'train': train_dataset, 'validation': val_dataset})\n","        cv_datasets.append(dataset_dict)\n","    \n","    return cv_datasets"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":153,"referenced_widgets":["b698f590c64b41f5a1dd4c0a08693cd1","6191df7925414c9487360ee6e862359c","f018480dca854a648dff942ed9db4ee3","ee6e0ab0c5d74b6d8110162aba877ea6","f675e3a8f6d449a086068819e6e92986","fcf20ef2bfd84174844cfc693709e729","39b58d56c9fb43f5962d3bfecb648766","a31b8ccfd53048c0ae38e090cfbcad78","541b9f0cc1ad4a9e99152a3de4245971","9642a250b24f400d8e950a7431d8fdc5","8feb7982963342f1b3bac9950a199b00"]},"editable":false,"id":"HG81D2lbCsQ_","outputId":"c2c97874-3d8c-4779-98c5-d2efcd6dedc4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn("]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b698f590c64b41f5a1dd4c0a08693cd1","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def prepare_dataset(tokenizer, dataset_dict=None, csv_path=None):\n","    \"\"\"Prepare dataset with option to pass DatasetDict directly or create from CSV\"\"\"\n","    if dataset_dict is None and csv_path is not None:\n","        # Original CSV loading logic\n","        df = pd.read_csv(csv_path)\n","        x = df['text'].copy()\n","        x_train, x_validation = train_test_split(x, test_size=0.2, random_state=RANDOM_SEED)\n","        \n","        # Reset indices\n","        x_train = x_train.reset_index(drop=True)\n","        x_validation = x_validation.reset_index(drop=True)\n","        \n","        # Convert to datasets\n","        train_dataset = Dataset.from_pandas(pd.DataFrame(x_train, columns=['text']))\n","        validation_dataset = Dataset.from_pandas(pd.DataFrame(x_validation, columns=['text']))\n","        \n","        dataset_dict = DatasetDict({'train': train_dataset, 'validation': validation_dataset})\n","    \n","    # Tokenize dataset\n","    tokenized_dataset = dataset_dict.map(\n","        lambda examples: prepare_train_features(examples, tokenizer),\n","        batched=True,\n","        remove_columns=dataset_dict[\"train\"].column_names,\n","        num_proc=12\n","    )\n","    \n","    return tokenized_dataset"]},{"cell_type":"code","execution_count":10,"metadata":{"editable":false,"id":"LIFjCCV_VMlk"},"outputs":[],"source":["def preprocess_logits_for_metrics(logits, labels):\n","    mlm_logits = logits[0]\n","    nsp_logits = logits[1]\n","    return mlm_logits.argmax(-1), nsp_logits.argmax(-1)\n","\n","\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","\n","    mlm_preds = preds[0]\n","    nsp_preds = preds[1]\n","\n","    mlm_labels = labels[0]\n","    nsp_labels = labels[1]\n","\n","    mask = mlm_labels != -100\n","    mlm_labels = mlm_labels[mask]\n","    mlm_preds = mlm_preds[mask]\n","\n","    mlm_accuracy = metric.compute(\n","        predictions=mlm_preds, references=mlm_labels)[\"accuracy\"]\n","    nsp_accuracy = metric.compute(\n","        predictions=nsp_preds, references=nsp_labels)[\"accuracy\"]\n","\n","    return {\"Masked ML Accuracy\": mlm_accuracy, \"NSP Accuracy\": nsp_accuracy}\n","\n","\n","def plot_losses(trainer):\n","    log_history = trainer.state.log_history\n","    df_history = pd.DataFrame(log_history)\n","    set_train_loss = []\n","    set_val_loss = []\n","    epochs = int(df_history['epoch'].max())\n","    for i in range(1, epochs+1):\n","        index_val = df_history.loc[(\n","            df_history['epoch'] == i), 'loss'].index.tolist()[0]\n","        index_train = index_val-1\n","        train_loss = df_history.loc[index_train, 'loss']\n","        val_loss = df_history.loc[index_val, 'eval_loss']\n","        set_train_loss.append(train_loss)\n","        set_val_loss.append(val_loss)\n","\n","    # Plot\n","    loss_index = range(1, len(set_train_loss) + 1)\n","\n","    plt.figure(figsize=(20, 5))\n","    plt.plot(loss_index, set_train_loss, label='Train loss')\n","    plt.plot(loss_index, set_val_loss, label='Val loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.xticks(range(1, len(set_train_loss) + 1))\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["f55c0afa874c46618d16591edc94efa8","0c54411b592244708711515f3be5987f","f229590e1c83477987a952b13137f01c","6df265e0210d49a288337bf784880dd2","6ac966fe50ec4cbcb25606ce99b436a1","f1c6f0e43e204021b79035332e483fc8","f389db0554dd40e8a12474cbf2499003","7887783911b34034a3216dc598512ab5","c30d446a43ed4566b823c9993ac42b9a","a4718b1501dc40619453ca968f8ea86d","11a17996326a40ea9de2350a72576ac8","51c42b5c14804b54b6cbf08d3a54bec4","d36af92611bf4be8b86e400f8a1f730f","b095296f2f39449897c9e12531197b7b","24ecc09e351e4e17986a75c4fc724292","0431da823a294721a9daf50bfd76a987","bcbd90f81cc54d3a8dcfbfd702ec1a27","da8ebaf21ff44a11a9b0c6e1a8f3fe40","085a6f8fa3be4bea8f3c7ebab97aa479","f31c42f174144ee691187ea1a99bbb35","bbc3828f9e1045928cf6d77b5fade272","ed1e8fe043664beb8cdc9d8397b5a642"]},"editable":false,"id":"Z-lggelPVM6i","outputId":"e1549a8a-1347-49cf-fd0b-31009928e606"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f55c0afa874c46618d16591edc94efa8","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=12):   0%|          | 0/4916 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51c42b5c14804b54b6cbf08d3a54bec4","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=12):   0%|          | 0/1230 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Initialize model and tokenizer\n","model, tokenizer = load_and_prepare_model()\n","\n","# Prepare dataset\n","tokenized_dataset = prepare_dataset(tokenizer, csv_path='unsupervised.csv')\n","\n","# Initialize data collator with NSP\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=True,\n","    mlm_probability=MLM_PROB\n",")\n"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"editable":false,"id":"ckNKtTyDWKLb","outputId":"90c00fd4-2542-4b11-e957-fa0e23bdcd29"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'token_type_ids', 'attention_mask', 'next_sentence_label'],\n","        num_rows: 4916\n","    })\n","    validation: Dataset({\n","        features: ['input_ids', 'token_type_ids', 'attention_mask', 'next_sentence_label'],\n","        num_rows: 1230\n","    })\n","})"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_dataset"]},{"cell_type":"code","execution_count":49,"metadata":{"editable":false,"id":"qS1nGCc2dUOq"},"outputs":[],"source":["learning_rates = [\n","    1e-4,\n","    1e-5,\n","    1e-6\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"editable":false,"id":"JauLXoLFV-4o","outputId":"530c145c-5eed-40b9-8007-930fa3ce0f5d"},"outputs":[],"source":["# Add this calculation before your training loop\n","total_train_samples = len(tokenized_dataset[\"train\"])\n","num_update_steps_per_epoch = total_train_samples // TRAIN_BATCH_SIZE\n","total_training_steps = num_update_steps_per_epoch * MAX_EPOCHS\n","warmup_steps = total_training_steps // 10  # 10% of total steps\n","for learning_rate in learning_rates:\n","  # Initialize Trainer\n","  training_args = TrainingArguments(\n","    output_dir=PATH_RESULT_MODEL,\n","    logging_first_step=True,\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=learning_rate,\n","    weight_decay=WEIGHT_DECAY,\n","    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n","    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n","    logging_steps=100,\n","    fp16=True,\n","    save_strategy=\"epoch\",\n","    num_train_epochs=MAX_EPOCHS,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_loss\",\n","    # Add these parameters\n","    warmup_steps=warmup_steps,\n","    lr_scheduler_type=\"linear\",\n","    gradient_clipping=1.0,  # Prevents exploding gradients\n",")\n","\n","  early_stop = EarlyStoppingCallback(early_stopping_patience = ES_PATIENCE, early_stopping_threshold = ES_THRESHOLD)\n","  trainer = Trainer(\n","      model=model,\n","      args=training_args,\n","      train_dataset=tokenized_dataset['train'],\n","      eval_dataset=tokenized_dataset['validation'],\n","      data_collator=data_collator,\n","      compute_metrics=compute_metrics,\n","      preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n","      callbacks=[early_stop]\n","  )\n","\n","  # Start training\n","  print(f\"Starting training..., LR = {learning_rate}\")\n","  trainer.train()\n","  plot_losses(trainer)\n","  # Save the model\n","  model.save_pretrained(f\"{PATH_RESULT_MODEL}/final_model_{learning_rate}\")\n","  tokenizer.save_pretrained(f\"{PATH_RESULT_MODEL}/final_model_{learning_rate}\")\n","  logger.info(\"Training completed and model saved\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the data once\n","df = pd.read_csv('unsupervised.csv')\n","cv_datasets = create_cv_datasets(df, n_splits=5)\n","\n","for learning_rate in learning_rates:\n","    cv_results = []\n","    \n","    for fold, dataset_dict in enumerate(cv_datasets):\n","        print(f\"\\nTraining fold {fold+1}/5 with learning rate {learning_rate}\")\n","        \n","        # Initialize fresh model for each fold\n","        model, tokenizer = load_and_prepare_model()\n","        \n","        # Prepare dataset for this fold\n","        tokenized_dataset = prepare_dataset(tokenizer, dataset_dict=dataset_dict)\n","        \n","        # Calculate steps (same as before)\n","        total_train_samples = len(tokenized_dataset[\"train\"])\n","        num_update_steps_per_epoch = total_train_samples // TRAIN_BATCH_SIZE\n","        total_training_steps = num_update_steps_per_epoch * MAX_EPOCHS\n","        warmup_steps = total_training_steps // 10\n","        \n","        training_args = TrainingArguments(\n","            output_dir=PATH_RESULT_MODEL,\n","            logging_first_step=True,\n","            evaluation_strategy=\"epoch\",\n","            learning_rate=learning_rate,\n","            weight_decay=WEIGHT_DECAY,\n","            per_device_train_batch_size=TRAIN_BATCH_SIZE,\n","            per_device_eval_batch_size=EVAL_BATCH_SIZE,\n","            logging_steps=100,\n","            fp16=True,\n","            save_strategy=\"epoch\",\n","            num_train_epochs=MAX_EPOCHS,\n","            load_best_model_at_end=True,\n","            metric_for_best_model=\"eval_loss\",\n","            # Add these parameters\n","            warmup_steps=warmup_steps,\n","            lr_scheduler_type=\"linear\",\n","                    gradient_clipping=1.0,  # Prevents exploding gradients\n","        )\n","        \n","        early_stop = EarlyStoppingCallback(\n","            early_stopping_patience=ES_PATIENCE, \n","            early_stopping_threshold=ES_THRESHOLD\n","        )\n","        \n","        trainer = Trainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=tokenized_dataset['train'],\n","            eval_dataset=tokenized_dataset['validation'],\n","            data_collator=data_collator,\n","            compute_metrics=compute_metrics,\n","            preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n","            callbacks=[early_stop]\n","        )\n","        \n","        # Train and evaluate\n","        train_result = trainer.train()\n","        cv_results.append(train_result.metrics)\n","        \n","        # Plot losses for this fold\n","        plot_losses(trainer)\n","        \n","        # Save the model for this fold\n","        model.save_pretrained(f\"{PATH_RESULT_MODEL}/final_model_lr{learning_rate}_fold{fold+1}\")\n","        tokenizer.save_pretrained(f\"{PATH_RESULT_MODEL}/final_model_lr{learning_rate}_fold{fold+1}\")\n","    \n","    # Calculate and log average metrics across folds\n","    avg_metrics = {k: np.mean([r[k] for r in cv_results]) for k in cv_results[0].keys()}\n","    logger.info(f\"Average metrics for learning rate {learning_rate}: {avg_metrics}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":6135391,"sourceId":9972436,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0431da823a294721a9daf50bfd76a987":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"085a6f8fa3be4bea8f3c7ebab97aa479":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c54411b592244708711515f3be5987f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1c6f0e43e204021b79035332e483fc8","placeholder":"â€‹","style":"IPY_MODEL_f389db0554dd40e8a12474cbf2499003","value":"Mapâ€‡(num_proc=12):â€‡100%"}},"11a17996326a40ea9de2350a72576ac8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24ecc09e351e4e17986a75c4fc724292":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbc3828f9e1045928cf6d77b5fade272","placeholder":"â€‹","style":"IPY_MODEL_ed1e8fe043664beb8cdc9d8397b5a642","value":"â€‡1230/1230â€‡[00:06&lt;00:00,â€‡189.58â€‡examples/s]"}},"39b58d56c9fb43f5962d3bfecb648766":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51c42b5c14804b54b6cbf08d3a54bec4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d36af92611bf4be8b86e400f8a1f730f","IPY_MODEL_b095296f2f39449897c9e12531197b7b","IPY_MODEL_24ecc09e351e4e17986a75c4fc724292"],"layout":"IPY_MODEL_0431da823a294721a9daf50bfd76a987"}},"541b9f0cc1ad4a9e99152a3de4245971":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6191df7925414c9487360ee6e862359c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcf20ef2bfd84174844cfc693709e729","placeholder":"â€‹","style":"IPY_MODEL_39b58d56c9fb43f5962d3bfecb648766","value":"Downloadingâ€‡builderâ€‡script:â€‡100%"}},"6ac966fe50ec4cbcb25606ce99b436a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6df265e0210d49a288337bf784880dd2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4718b1501dc40619453ca968f8ea86d","placeholder":"â€‹","style":"IPY_MODEL_11a17996326a40ea9de2350a72576ac8","value":"â€‡4916/4916â€‡[00:29&lt;00:00,â€‡292.19â€‡examples/s]"}},"7887783911b34034a3216dc598512ab5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8feb7982963342f1b3bac9950a199b00":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9642a250b24f400d8e950a7431d8fdc5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a31b8ccfd53048c0ae38e090cfbcad78":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4718b1501dc40619453ca968f8ea86d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b095296f2f39449897c9e12531197b7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_085a6f8fa3be4bea8f3c7ebab97aa479","max":1230,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f31c42f174144ee691187ea1a99bbb35","value":1230}},"b698f590c64b41f5a1dd4c0a08693cd1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6191df7925414c9487360ee6e862359c","IPY_MODEL_f018480dca854a648dff942ed9db4ee3","IPY_MODEL_ee6e0ab0c5d74b6d8110162aba877ea6"],"layout":"IPY_MODEL_f675e3a8f6d449a086068819e6e92986"}},"bbc3828f9e1045928cf6d77b5fade272":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcbd90f81cc54d3a8dcfbfd702ec1a27":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c30d446a43ed4566b823c9993ac42b9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d36af92611bf4be8b86e400f8a1f730f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcbd90f81cc54d3a8dcfbfd702ec1a27","placeholder":"â€‹","style":"IPY_MODEL_da8ebaf21ff44a11a9b0c6e1a8f3fe40","value":"Mapâ€‡(num_proc=12):â€‡100%"}},"da8ebaf21ff44a11a9b0c6e1a8f3fe40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed1e8fe043664beb8cdc9d8397b5a642":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee6e0ab0c5d74b6d8110162aba877ea6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9642a250b24f400d8e950a7431d8fdc5","placeholder":"â€‹","style":"IPY_MODEL_8feb7982963342f1b3bac9950a199b00","value":"â€‡4.20k/4.20kâ€‡[00:00&lt;00:00,â€‡88.7kB/s]"}},"f018480dca854a648dff942ed9db4ee3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a31b8ccfd53048c0ae38e090cfbcad78","max":4203,"min":0,"orientation":"horizontal","style":"IPY_MODEL_541b9f0cc1ad4a9e99152a3de4245971","value":4203}},"f1c6f0e43e204021b79035332e483fc8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f229590e1c83477987a952b13137f01c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7887783911b34034a3216dc598512ab5","max":4916,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c30d446a43ed4566b823c9993ac42b9a","value":4916}},"f31c42f174144ee691187ea1a99bbb35":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f389db0554dd40e8a12474cbf2499003":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f55c0afa874c46618d16591edc94efa8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0c54411b592244708711515f3be5987f","IPY_MODEL_f229590e1c83477987a952b13137f01c","IPY_MODEL_6df265e0210d49a288337bf784880dd2"],"layout":"IPY_MODEL_6ac966fe50ec4cbcb25606ce99b436a1"}},"f675e3a8f6d449a086068819e6e92986":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcf20ef2bfd84174844cfc693709e729":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":4}
